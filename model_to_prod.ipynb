{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADDING PREPSRATION LAYERS TO FORWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pretrained models\n",
    "eng_model = torch.load('models/ENG_PROD.pth')\n",
    "eng_model.cpu()\n",
    "eng_model.eval()\n",
    "\n",
    "rus_model = torch.load('models/RUS_PROD.pth')\n",
    "rus_model.cpu()\n",
    "rus_model.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngModelProd(nn.Module):\n",
    "    '''For cropped'''\n",
    "    def __init__(self, original_model):\n",
    "        super(EngModelProd, self).__init__()\n",
    "        self.features = nn.Sequential(*list(original_model.features))\n",
    "        self.avgpool = nn.Sequential(original_model.avgpool)\n",
    "        self.classifier = nn.Sequential(*list(original_model.classifier))\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngModelProd(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(EngModelProd, self).__init__()\n",
    "        self.features = nn.Sequential(*list(original_model.features))\n",
    "        self.avgpool = nn.Sequential(original_model.avgpool)\n",
    "        self.classifier = nn.Sequential(*list(original_model.classifier))\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(4, 720, 1280)\n",
    "        x = x[:3, :, :]\n",
    "        x = F.pad(x, mode='replicate', pad=(860, 860, 580, 580))\n",
    "        x = nn.functional.avg_pool2d(x, kernel_size=10, stride=(10, 10))\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageModel(nn.Module):\n",
    "    '''USING IMAGE FROM PATH'''\n",
    "    def __init__(self, original_model):\n",
    "        super(ImageModel, self).__init__()\n",
    "        self.features = nn.Sequential(*list(original_model.features))\n",
    "        self.avgpool = nn.Sequential(original_model.avgpool)\n",
    "        self.classifier = nn.Sequential(*list(original_model.classifier))\n",
    "    \n",
    "    def forward(self, x: str) -> torch.Tensor:\n",
    "        x = Image.open(x)\n",
    "        x = transforms.Resize((244, 244))(x)\n",
    "        x = x.reshape(4, 720, 1280)\n",
    "        x = x[:3, :, :]\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformsModel(nn.Module):\n",
    "    '''Standard model with input transformation'''\n",
    "    \n",
    "    def __init__(self, original_model):\n",
    "        super(TransformsModel, self).__init__()\n",
    "        self.features = nn.Sequential(*list(original_model.features))\n",
    "        self.avgpool = nn.Sequential(original_model.avgpool)\n",
    "        self.classifier = nn.Sequential(*list(original_model.classifier))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = transforms.Resize(244)(x)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_md_prod = TransformsModel(eng_model)\n",
    "rus_md_prod = TransformsModel(rus_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing hardswish with custom one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class New_Hardswish(nn.Module):\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return x * F.hardtanh(x + 3, 0.0, 6.0) / 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_layers(model, old, new):\n",
    "    for n, module in model.named_children():\n",
    "        if len(list(module.children())) > 0:\n",
    "            replace_layers(module, old, new)\n",
    "            \n",
    "        if isinstance(module, old):\n",
    "            setattr(model, n, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_layers(eng_md_prod, nn.Hardswish, New_Hardswish())\n",
    "replace_layers(rus_md_prod, nn.Hardswish, New_Hardswish())\n",
    "\n",
    "eng_md_prod.load_state_dict(torch.load('models/eng_md_prod'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus_md_prod.load_state_dict(torch.load('models/rus_new_dataset'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPORTING TO ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nikita\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.ones(3, 720, 1280)\n",
    "\n",
    "eng_md_prod.cpu()\n",
    "eng_md_prod.eval()\n",
    "torch.onnx.export(eng_md_prod,\n",
    "                    dummy_input,\n",
    "                    'onnx_models/ENG_MD_PROD.onnx',\n",
    "                    export_params=True,\n",
    "                    do_constant_folding=False,\n",
    "                    input_names = ['input'],\n",
    "                    output_names = ['output'])\n",
    "\n",
    "rus_md_prod.cpu()\n",
    "rus_md_prod.eval()\n",
    "torch.onnx.export(rus_md_prod,\n",
    "                    dummy_input,\n",
    "                    'onnx_models/RUS_MD_PRODv2.onnx',\n",
    "                    export_params=True,\n",
    "                    do_constant_folding=False,\n",
    "                    input_names = ['input'],\n",
    "                    output_names = ['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"input\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 3\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 720\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 1280\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = onnx.load('onnx_models/ENG_MD_PROD.onnx')\n",
    "output = model.graph.output\n",
    "\n",
    "input_all = model.graph.input\n",
    "input_initializer = model.graph.initializer\n",
    "input_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"input\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 3\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 720\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 1280\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = onnx.load('onnx_models/RUS_MD_PRODv2.onnx')\n",
    "output = model.graph.output\n",
    "\n",
    "input_all = model.graph.input\n",
    "input_initializer = model.graph.initializer\n",
    "input_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
